{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN  - This is a Multilayer Perceptron (MLP) example using Keras\n",
    "\n",
    "    Copyright (C) 2020 Adrian Bevan, Queen Mary University of London\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "    \n",
    "----------------------\n",
    "\n",
    "## Useful background on MNIST and data wrangling\n",
    "\n",
    "MLP example using a Keras model. This is a simple example of a 2 layer MLP processing the MNIST data using the Adam optimiser.  The MNIST data is a set of 60,000 training and 10,000 test examples.  Each example is a hand written integer between 0 and 9, represented by a greyscale image with 28x28 pixels. This constitutes a 784 dimensional input feature space, where each image is a number between 0 and 255 (8 bit greyscale).  In order for an image to be processed efficiently by a neural network the pixel colour in the range $[0, 255]$ is mapped into the domain $[0, 1]$, following the usual [Efficient Backpropagation](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) data wrangling guidelines. For an MLP the 28x28 pixel image features (i.e. the pixels) are considered a linear set of 784 features that are input to the network.\n",
    "\n",
    "For more information about the MNIST data set please see the website http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "For more information about the Adam optimiser please see the  paper by Kingma and Ba, [arXiv:1412.6980](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "----------------------\n",
    "## Load and pre-process the data\n",
    "\n",
    "The MNIST data are directly accessible via keras as a dataset. So we first load the data. As noted above (and as you will have explored in the linear regression example), it is important to closely match the weights used in a network with the feature space domain that is being studied, so that the optimiser has less work to do in order to converge to the optimal solution.  In this case we achieve that by mapping the 8-bit greyscale color value $[0, 255]$ on the domain $[0, 1]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MNIST data from keras\n",
      "\tN(test) =  60000\n",
      "\tN(test) =  10000\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Loading the MNIST data from keras\")\n",
    "# Load the MNIST data via the tensorflow keras dataset:\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(\"\\tN(test) = \", len(x_train))\n",
    "print(\"\\tN(test) = \", len(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Building the model \n",
    "\n",
    "**Training [Epochs, batches and validation data]** The number of training epochs specified is denoted by `Nepochs`.  1 epoch is required to run over all of the training data.  It is possible to run batches or mini-batches of data through the training; each batch requires the optimisation to be performed, and so when specifying the `BatchSize` the training will be performed by running the optimisation $N_{epochs}\\timesN_{batches}$ times.  In general this leads to faster optimisation of the model than optimising over the full training set each time.\n",
    "\n",
    "The training data will be split into training and validation samples according to the value of the variable `ValidationSplit`. \n",
    "\n",
    "**Dropout:** Coadaptation is the ability for an optimisation algorithm to allow weights to be learned where changes in one node can be compensated by changes in another node that limit the increase in performance.  This issue can be a problem for deep networks in particular where the optimisation process can involve millions of hyperparameters.  A way to combat this issue is to randomly drop-out nodes in the network each iteration of the optimisation.  That way no single paring of nodes can learn to co-adapt to the evolution of hyperparameters through the optimisation.  Thankfully all the user has to do is to set a dropout value via the variable `DropoutValue`. \n",
    "\n",
    "**NOTE:** this value is the fraction of nodes dropped from the model [In V1.X of TensorFlow the opposite convention was used].\n",
    "\n",
    "**Loss:** The cross entropy loss function is used for this optimisation process.  The value of the loss function is converted into an output vector of 1's and 0's to be used for classification.\n",
    "\n",
    "### Model configuration\n",
    "\n",
    "The MLP consists of an input layer, two hidden layers, in this case one drop out layer and finally an output layer. These are as follows:\n",
    "- **Inputs:** This model specifies an input shape of 28x28 that is flattened. The purpose of this is to ensure that there are 784 input features being fed into the hidden layers of the network.\n",
    "- **Hidden Layers:** This network has two hidden layers, wich using a leaky ReLU (Rectified Linear Unit) activation function.  The parameter alpha defines the slope of the function for negative values, and for positive values the activation function is simply $f(x)=x$.\n",
    "- **Dropout Layer:** Here the only dropout layer is the one from the second hidden layer of the network to the output.  You may wish to explore what happens if another dropout layer is added after the first hidden layer.\n",
    "- **Output:** The output is a vector of 10 numbers. As the loss function is a cross entropy loss function the output in this case will be a one hot vector, i.e. a vector of 10 digits that are either 0 or 1.  The element corresponding to 1 being the optimal classificaiton of the example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bevan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Will train a multilayer perceptron on MNIST data\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Input data MNIST\n",
      "2 layer MLP with configuration 784:128:128:10\n",
      "Dropout values       =  0.6\n",
      "Leaky relu parameter =  0.1\n",
      "ValidationSplit      =  0.5\n",
      "BatchSize            =  20\n",
      "Nepochs              =  20 \n",
      "\n",
      "N(train)             =  60000\n",
      "N(test)              =  10000\n",
      "Train on 30000 samples, validate on 30000 samples\n",
      "Epoch 1/20\n",
      "30000/30000 [==============================] - 3s 94us/sample - loss: 0.4308 - acc: 0.8707 - val_loss: 0.1855 - val_acc: 0.9449\n",
      "Epoch 2/20\n",
      "30000/30000 [==============================] - 2s 70us/sample - loss: 0.1934 - acc: 0.9451 - val_loss: 0.1461 - val_acc: 0.9558\n",
      "Epoch 3/20\n",
      "30000/30000 [==============================] - 2s 71us/sample - loss: 0.1439 - acc: 0.9581 - val_loss: 0.1336 - val_acc: 0.9594\n",
      "Epoch 4/20\n",
      "30000/30000 [==============================] - 2s 77us/sample - loss: 0.1126 - acc: 0.9660 - val_loss: 0.1325 - val_acc: 0.9626\n",
      "Epoch 5/20\n",
      "30000/30000 [==============================] - 2s 75us/sample - loss: 0.0950 - acc: 0.9710 - val_loss: 0.1327 - val_acc: 0.9627\n",
      "Epoch 6/20\n",
      "30000/30000 [==============================] - 2s 74us/sample - loss: 0.0828 - acc: 0.9747 - val_loss: 0.1199 - val_acc: 0.9676\n",
      "Epoch 7/20\n",
      "30000/30000 [==============================] - 2s 81us/sample - loss: 0.0733 - acc: 0.9768 - val_loss: 0.1345 - val_acc: 0.9629\n",
      "Epoch 8/20\n",
      "30000/30000 [==============================] - 2s 74us/sample - loss: 0.0604 - acc: 0.9801 - val_loss: 0.1697 - val_acc: 0.9646\n",
      "Epoch 9/20\n",
      "30000/30000 [==============================] - 2s 76us/sample - loss: 0.0590 - acc: 0.9820 - val_loss: 0.1479 - val_acc: 0.9664\n",
      "Epoch 10/20\n",
      "30000/30000 [==============================] - 2s 78us/sample - loss: 0.0523 - acc: 0.9830 - val_loss: 0.1295 - val_acc: 0.9683\n",
      "Epoch 11/20\n",
      "30000/30000 [==============================] - 2s 72us/sample - loss: 0.0520 - acc: 0.9834 - val_loss: 0.1316 - val_acc: 0.9691\n",
      "Epoch 12/20\n",
      "30000/30000 [==============================] - 2s 73us/sample - loss: 0.0445 - acc: 0.9860 - val_loss: 0.1406 - val_acc: 0.9702\n",
      "Epoch 13/20\n",
      "30000/30000 [==============================] - 2s 73us/sample - loss: 0.0377 - acc: 0.9876 - val_loss: 0.1557 - val_acc: 0.9685\n",
      "Epoch 14/20\n",
      "30000/30000 [==============================] - 2s 71us/sample - loss: 0.0387 - acc: 0.9868 - val_loss: 0.1519 - val_acc: 0.9686\n",
      "Epoch 15/20\n",
      "30000/30000 [==============================] - 2s 71us/sample - loss: 0.0393 - acc: 0.9875 - val_loss: 0.1777 - val_acc: 0.9656\n",
      "Epoch 16/20\n",
      "30000/30000 [==============================] - 2s 71us/sample - loss: 0.0343 - acc: 0.9887 - val_loss: 0.1763 - val_acc: 0.9655\n",
      "Epoch 17/20\n",
      "30000/30000 [==============================] - 2s 71us/sample - loss: 0.0335 - acc: 0.9889 - val_loss: 0.1759 - val_acc: 0.9698\n",
      "Epoch 18/20\n",
      "30000/30000 [==============================] - 2s 70us/sample - loss: 0.0360 - acc: 0.9887 - val_loss: 0.1808 - val_acc: 0.9670\n",
      "Epoch 19/20\n",
      "30000/30000 [==============================] - 2s 71us/sample - loss: 0.0355 - acc: 0.9891 - val_loss: 0.1822 - val_acc: 0.9692\n",
      "Epoch 20/20\n",
      "30000/30000 [==============================] - 2s 74us/sample - loss: 0.0339 - acc: 0.9898 - val_loss: 0.1826 - val_acc: 0.9700\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Training configuration\n",
    "#\n",
    "ValidationSplit = 0.5\n",
    "BatchSize       = 20\n",
    "Nepochs         = 20\n",
    "DropoutValue    = 0.6\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),  #Â MNIST is 28x28 pixel images\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "  tf.keras.layers.Dropout(DropoutValue),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Will train a multilayer perceptron on MNIST data\")\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Input data MNIST\")\n",
    "print(\"2 layer MLP with configuration 784:128:128:10\")\n",
    "print(\"Dropout values       = \", DropoutValue)\n",
    "print(\"Leaky relu parameter =  0.1\")\n",
    "print(\"ValidationSplit      = \", ValidationSplit)\n",
    "print(\"BatchSize            = \", BatchSize)\n",
    "print(\"Nepochs              = \", Nepochs, \"\\n\")\n",
    "print(\"N(train)             = \", len(x_train))\n",
    "print(\"N(test)              = \", len(x_test))\n",
    "\n",
    "# now specify the loss function - cross entropy\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "# now we can train the model to make predictions.\n",
    "#   Use the ADAM optimiser\n",
    "#   Specify the metrics to report as accuracy\n",
    "#   Specify the loss function (see above)\n",
    "# the fit step specifies the number of training epochs\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "history  = model.fit(x_train, y_train, validation_split=ValidationSplit, batch_size=BatchSize, epochs=Nepochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history keys =  dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "\n",
      "Display the evolution of the accuracy as a function of the training epoch\n",
      "  N(Epochs)        =  20\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-822c59034fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDisplay the evolution of the accuracy as a function of the training epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  N(Epochs)        = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  accuracy (train) = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  accuracy (test)  = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "# Print out the history keys expected are:\n",
    "#    loss        The loss function evaluated at each epoch for the training set\n",
    "#    acc         The accuracy evaluated at each epoch for the training set\n",
    "#    val_loss    The loss evaluated at each epoch for the validation set\n",
    "#    val_acc     The accuracy evaluated at each epoch for the validation set\n",
    "# The val_* entries exist only if there is a validation_split specified\n",
    "print(\"history keys = \", history.history.keys())\n",
    "\n",
    "print(\"\\nDisplay the evolution of the accuracy as a function of the training epoch\")\n",
    "print(\"  N(Epochs)        = \", Nepochs)\n",
    "print(\"  accuracy (train) = \", history.history['accuracy'])\n",
    "print(\"  accuracy (test)  = \", history.history['val_accuracy'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.show()\n",
    "print(\"Plotting the output to fig/NN_MLP_accuracy_vs_epochs.*\")\n",
    "plt.savefig(\"fig/NN_MLP_accuracy_vs_epochs.pdf\")\n",
    "plt.savefig(\"fig/NN_MLP_accuracy_vs_epochs.png\")\n",
    "plt.clf()\n",
    "\n",
    "print(\"\\nDisplay the evolution of the loss as a function of the training epoch\")\n",
    "print(\"  N(Epochs)        = \", Nepochs)\n",
    "print(\"  loss (train)     = \", history.history['loss'])\n",
    "print(\"  loss (test)      = \", history.history['val_loss'])\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.show()\n",
    "print(\"Plotting the output to fig/NN_MLP_loss_vs_epochs.*\")\n",
    "plt.savefig(\"fig/NN_MLP_loss_vs_epochs.pdf\")\n",
    "plt.savefig(\"fig/NN_MLP_loss_vs_epochs.png\")\n",
    "\n",
    "# having finished training the model, use this to evaluate the performance on a sample of test data\n",
    "print(\"\\nPerformance summary (on test data):\")\n",
    "loss, acc = model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(\"\\tloss = {:5.3f}\\n\\taccuracy = {:5.3f}\".format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
